\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\begin{document}
 
\title{Week 4}
\author{Josh Klontz\\CSE 891}
 
\maketitle
 
The papers this week explore modern alternatives to active appearance models for face alignment.
The first paper is the work of Wu et al.\ on ``Face Alignment via Boosted Ranking Model'' which formulates face alignment as a rank-learning problem.
The second paper, ``Principal Regression Analysis'' by Saragih, proposes alignment as a local optimization task in a subspace believed to contain the correct response.
The third paper by Cao et al. titled, ``Face Alignment by Explicit Shape Regression'', moves away from parametric shape models entirely and demonstrates high speed and accuracy using boosted regression and a cascading learning framework.
\par
The primary advantage of the Boosted Ranking Model over an Active Appearance Model is that the alignment problem generalizes much better to faces outside the training set.
This is accomplished by formulating the task of finding a better alignment given the current alignment in a way that is invariant to an individual's identity.
In particular, this is accomplished by boosted learning on Haar-like features.
The features that are chosen are the ones that best serve the task of finding a convex score function such that, in a local neighborhood, a higher score indicates an alignment closer to the optimal alignment.
\par
The Boosted Ranking Model appears to have at least the following two disadvantages. First, the authors motivate the choice of Haar-like features for their computational efficiency and their success in face-related applications. While the success of Haar-like features in face-related applications is indisputable, the nature of the alignment problem as formulated does not appear to take advantage of the computational efficiency of the feature set. The integral image trick allows Haar-like features to be extracted in constant time at different scales. However, since the face images in this paper are already scale-normalized and discarded after each iteration, it would seem that a more sophisticated, and ideally representative, feature set could be utilized with negligible impact on runtime performance. Second, while the runtime speed isn't mentioned, one wonders the computational cost of a piecewise affine warp at each iteration. Especially when the algorithm is designed to make small increments toward the optimal alignment in each step.
\par
Principal Regression Analysis offers a complementary approach to conventional alignment algorithms.
The proposed approach is novel in that, unlike conventional regression, PRA does not identify the target output but rather generates a low dimensional subspace thought to contain the target.
The low dimensional subspace can then be used as the search space by other image alignment algorithms.
Effectively, the approach offers a ``strong prior'' that can be leveraged by subsequent algorithms.
\par
The primary disadvantage to the approach is that PRA involves minimizing an objective function over a high-dimensional non-convex surface.
As a result, the techniques proposed for solving the problem are highly complex and computationally demanding.
\par
The final work termed ``Explicit Shape Regression'' offers an alternative approach to alignment whereby instead of learning a PCA subspace of viable shapes, the regressed shape is always a linear combination of the training shapes.
In addition to this departure from conventional approaches, the proposed approach also exhibits unusual computation efficiency and is a thousand times faster than the next most accurate approach.
The foundation of the algorithm is cascading framework where early steps in the cascade learn rough alignment changes and later steps fine-tune the model fit.
Random pixel-difference features are used for their computational simplicity and are selected based on their descriminability and orthogonality.
I could identify no major objections to this approach.
 
\end{document}
