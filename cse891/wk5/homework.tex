\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\begin{document}
 
\title{Week 5}
\author{Josh Klontz\\CSE 891}
 
\maketitle
 
The papers this week explore Bayesian inspired approaches to comparing faces.
The first paper is the classic work of Moghaddam et al.\ on ``Bayesian face recognition'' which demonstrates the Mahalanobis distance as more effective than the Euclidean distance for comparing faces.
The second paper, ``Probabilistic Models for Inference about Identity'' by Li et al., suggests learning identity as a latent variable influenced by presentation and noise.
The third paper by Chen et al. titled, ``Bayesian Face Revisited: A Join Formulation'', extends Moghaddam's work by considering a joint model and using the likelihood ratio as the similarity metric.
\par
The seminal work of Moghaddam et al.\ on Bayesian face recognition presents a probabilistic measure of similarity based on a MAP analysis of image differences.
The paper offers several benifits to the computer vision field.
First, the proposed technique is generic and can be used under any image matching scenario involving the difference of two real-valued vectors.
The approach demonstrates improved accuracy and generality arising from the use of the Mahalanobis distance instead of the Euclidean distance.
Second, the resulting similarity score has an interpretable meaning as the probability of a genuine match given equal priors.
Third, the authors demonstrate how the approach can be optimized so the comparison is almost as fast as a Euclidean distance.
\par
While Moghaddam's approach was shown to be effective at modeling expression and lighting, it is less clear that the same technique can be extended to model pose or facial decorations.
The fundamental issue is that incorporating such gross variations as pose and extreme decorations significantly increases the dimensionality of the subspaces, effectively diluting the density models and rendering them ineffective.
Instead, the authors suggest the use of a multiple-model approach for handling these variations.
\par
The fundamental contribution of the work done by Li et al.\ is a model of the form $x_{ij}=f(h_i,w_{ij},\theta)+\epsilon_{ij}$.
Here $x_{ij}$ is the vectorized data from the $j$th image of the $i$th person, $h_i$ is the latent identity variable, $w_{ij}$ is the viewing conditions, $\theta$ is a vector of model parameters, and $\epsilon_{ij}$ is a Gaussian noise term.
The authors then investigate two approaches to computing face similarity by: 1) evaluating the joint probability of probe and gallery images and 2) forming class-conditional predictive distributions.
Both approaches are considered in three increasingly sophisticated models: 1) probabilistic LDA, 2) Mixtures of PLDAs, and 3) Tied PLDA.
\par
There are two primary weaknesses to the paper.
The first is that the proposed approaches lack closed form solutions, though the authors demonstrate Expectation Maximization as an effective parameter learning technique.
The second is that the paper does not leverage modern successes in patch based feature representations, and instead demonstrates results on raw pixel intensity only.
\par
The primary disadvantage to the approach is that PRA involves minimizing an objective function over a high-dimensional non-convex surface.
As a result, the techniques proposed for solving the problem are highly complex and computationally demanding.
\par
The final work termed ``Explicit Shape Regression'' offers an alternative approach to alignment whereby instead of learning a PCA subspace of viable shapes, the regressed shape is always a linear combination of the training shapes.
In addition to this departure from conventional approaches, the proposed approach also exhibits unusual computation efficiency and is a thousand times faster than the next most accurate approach.
The foundation of the algorithm is cascading framework where early steps in the cascade learn rough alignment changes and later steps fine-tune the model fit.
Random pixel-difference features are used for their computational simplicity and are selected based on their descriminability and orthogonality.
I could identify no major objections to this approach.
 
\end{document}
